# CryptoPulse
## Real-Time Cryptocurrency Arbitrage Detection Lakehouse

**CryptoPulse** is a real-time data engineering project built using **Kappa Architecture** and **Lakehouse design principles**. It ingests live cryptocurrency trade data, guarantees durability with Kafka, and processes streams with Spark Structured Streaming to detect arbitrage opportunities while maintaining high-quality historical data for analytics and backtesting.

--
## ğŸ“š Table of Contents


[Problem Statement](#-Problem-Statement)

[Solution Overview](#-Solution-Overview)

[Architecture Diagram](#-Architecture-Diagram)

[Project Structure](#-Project-Structure)

[Pipeline Layers](#-Pipeline-Layers)

[Ingestion Layer Producers](#-Ingestion-Layer-Producers)

[Bronze Layer â€” Raw Data](#-Bronze-Layerâ€”Raw-Data)

[Silver Layer â€” Cleaned Data](#-Silver-Layerâ€”Cleaned-Data)

Gold Layer â€” Aggregated Metrics

Core Data Flow Diagram

Tech Stack

Key Engineering Decisions

Why Delta Lake?

Why Local Spark Instead of Databricks Community Edition?

Schema Evolution Strategy

Features

Setup & Installation

Demo

---

## ğŸ“Œ Problem Statement

Cryptocurrency markets are highly fragmented. The same asset frequently trades at different prices across exchanges due to liquidity differences, latency, and market inefficiencies.

Detecting arbitrage opportunities is challenging because:
- Data volume is extremely high
- Exchange APIs are unstable and disconnect frequently
- Events arrive out of order due to network latency
- Streaming systems must guarantee zero data loss

---

## ğŸ’¡ Solution Overview

CryptoPulse decouples ingestion from processing using **Apache Kafka** and processes data using **Spark Structured Streaming** backed by **Delta Lake**. This design ensures:

- Fault-tolerant ingestion
- Exactly-once processing semantics
- Correct handling of late-arriving data
- Replayable raw data for reprocessing and schema evolution

---

## ğŸ— Architecture Diagram
<p align="center">
  <img src="images/architecture_crypto.png" width="900">
</p>

---
## ğŸ“ Project Structure

```text
.crypto-pulse/
â”œâ”€â”€ data/                         # Local Data Lake
â”‚   â”œâ”€â”€ delta/                    # ACID Tables (Bronze / Silver / Gold)
â”‚   â””â”€â”€ checkpoints/              # Spark Streaming Checkpoints
â”œâ”€â”€ logs/                         # Runtime logs
â”œâ”€â”€ images/                       # Architecture & Pipeline Screenshots
â”‚   â”œâ”€â”€ architecture_crypto.png
â”‚   â”œâ”€â”€ dataflow_diagram_crypto.png
â”‚   â”œâ”€â”€ kafka_confluent_topic_lineage.png
â”‚   â”œâ”€â”€ kafka_messages.png
â”‚   â”œâ”€â”€ gold_layer_data.png
â”‚   â”œâ”€â”€ live_dashboard.png
â”‚   â””â”€â”€ orchestrator_screenshot.png
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ dashboard/
â”‚   â”‚   â””â”€â”€ app.py                # Streamlit Visualization
â”‚   â”œâ”€â”€ jobs/
â”‚   â”‚   â”œâ”€â”€ ingestion_bronze.py   # Kafka â†’ Bronze (Delta)
â”‚   â”‚   â”œâ”€â”€ process_silver.py     # Bronze â†’ Silver (Cleaning)
â”‚   â”‚   â”œâ”€â”€ gold_layer.py         # Silver â†’ Gold (Aggregation)
â”‚   â”‚   â””â”€â”€ orchestrator.py       # Pipeline Orchestration
â”‚   â”œâ”€â”€ producers/
â”‚   â”‚   â””â”€â”€ main.py               # WebSocket â†’ Kafka Producer
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ check_bronze.py
â”‚   â”‚   â””â”€â”€ check_gold.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ .env                          # Secrets (Git ignored)
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
```

The pipeline follows a **Lakehouse Medallion Architecture**.

---

## ğŸ”¹ Pipeline Layers

### 1. Ingestion Layer Producers
- **Source:** Coinbase & Binance WebSocket APIs
- **Language:** Python
- **Transport:** Apache Kafka (Confluent Cloud)

Kafka provides a durable buffer between volatile APIs and Spark, ensuring no data loss if downstream systems fail.

---

### 2. Bronze Layer â€” Raw Data
- **Storage:** Delta Lake
- **Write Mode:** Append-only
- **Schema:** Raw JSON (`CAST(value AS STRING)`)

**Why this matters**
- Enables full stream replay
- Protects against API schema changes
- Decouples ingestion from parsing logic

---

### 3. Silver Layer â€” Cleaned Data
- JSON parsing and schema enforcement
- Deduplication
- Type casting

**Financial Precision**
Prices are stored as `Decimal(18,8)` instead of floating-point doubles to avoid rounding errors common in financial calculations.

---

### 4. Gold Layer â€” Aggregated Metrics
- 1-minute OHLC candles
- VWAP (Volume Weighted Average Price)
- Windowing: 1 minute
- Watermarking: 10 seconds

Late-arriving events are correctly assigned to their original window before aggregation is finalized.

---
## ğŸ§© Core Data Flow Diagram 
<p align="center">
  <img src="images/dataflow_diagram_crypto.png" width="900">
</p>

## ğŸ›  Tech Stack

### Core Technologies
- **Language:** Python 3.12
- **Streaming Engine:** Apache Spark 3.5.3
- **Message Broker:** Apache Kafka (Confluent Cloud)
- **Storage Format:** Delta Lake 3.3.0
- **Architecture:** Kappa + Lakehouse

### Key Libraries
- `pyspark`
- `delta-spark`
- `confluent-kafka`
- `uv`

---

## ğŸ§  Key Engineering Decisions

### Why Delta Lake?
Traditional Parquet streaming pipelines often suffer from:
- Small file problems
- Concurrent read/write conflicts
- Downstream corruption

Delta Lake introduces a transaction log (`_delta_log`) that provides:
- ACID guarantees
- Safe concurrent reads and writes
- Reliable downstream consumption

---

### Why Local Spark Instead of Databricks Community Edition?
Databricks Community Edition:
- Blocks outbound Kafka connections
- Imposes execution limits

Running Spark locally allows:
- Full JVM control
- Unlimited execution time
- Use of the same Delta Lake APIs used in production environments

---

### Schema Evolution Strategy
By storing raw JSON in the Bronze layer:
- Schema changes do not break the pipeline
- Parsing logic can evolve independently
- Historical data can be reprocessed safely

---

## ğŸš€ Features

- Fault-tolerant ingestion with Kafka buffering
- ACID-compliant streaming analytics
- Stateful processing with watermarking and windowing
- Replayable raw data
- Financial-grade numeric precision
- Modern Python dependency management with `uv`

---

## ğŸ’» Setup & Installation

### Prerequisites
- Python 3.10+
- Java 17 (required for Spark)
- Confluent Cloud account (free tier)
- `uv` package manager

```bash
pip install uv
```

ğŸ“¸ Demo
Real-Time Dashboard
Here is the live Streamlit dashboard visualizing the 1-minute OHLC candles from the Gold layer.
<p align="center">
  <img src="images/live_dashobard,png.png" width="900">
</p>

Pipeline Orchestration
The custom Python orchestrator managing 4 concurrent Spark jobs. !
<p align="center">
  <img src="images/orchestrator_Screenshot.png" width="900">
</p>

Kafka Confluent Data Lineage Graph
<p align="center">
  <img src="images/kafka_confluent_topic_lineage.png" width="900">
</p> 




